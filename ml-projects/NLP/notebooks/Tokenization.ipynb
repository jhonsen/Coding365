{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: Natural Language Processing with Java Cookbook [Packt](https://www.packtpub.com/product/natural-language-processing-with-java-cookbook/9781789801156)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing with OpenNLP Library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load POM dependencies from [OpenNLP](https://opennlp.apache.org/maven-dependency.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<repositories>\n",
    "  <repository>\n",
    "    <id>apache opennlp snapshot</id>\n",
    "    <url>https://repository.apache.org/content/repositories/snapshots/</url>\n",
    "  </repository>\n",
    "</repositories>\n",
    "\n",
    "<dependency>\n",
    "    <groupId>org.apache.opennlp</groupId>\n",
    "    <artifactId>opennlp-tools</artifactId>\n",
    "    <version>1.9.0</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use **SimpleTokenizer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "the\n",
      "best\n",
      "day\n",
      "of\n",
      "my\n",
      "life\n",
      ",\n",
      "as\n",
      "some\n",
      "would\n",
      "say\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.tokenize.SimpleTokenizer;\n",
    "\n",
    "public void tokenizeSentence(String sentence) {\n",
    "    SimpleTokenizer simpletkn = SimpleTokenizer.INSTANCE;\n",
    "    String tokenList[] = simpletkn.tokenize(sentence);\n",
    "    for (String token: tokenList) {\n",
    "        System.out.println(token);\n",
    "    }\n",
    "}\n",
    "\n",
    "String phrase = \"This is the best day of my life, as some would say.\";\n",
    "tokenizeSentence(phrase);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing with OpenNLP's Maximum Entropy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.FileNotFoundException;\n",
    "import java.io.IOException;\n",
    "import java.io.File;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.InputStream;\n",
    "import opennlp.tools.tokenize.Tokenizer;\n",
    "import opennlp.tools.tokenize.TokenizerME;\n",
    "import opennlp.tools.tokenize.TokenizerModel;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "the\n",
      "best\n",
      "day\n",
      "indeed\n",
      "!\n"
     ]
    }
   ],
   "source": [
    "public void tokenizeMaxEntropy(String phrase){\n",
    "    try (InputStream modelInputStream = new FileInputStream(new File(\"../models/\", \"en-token.bin\"))) {\n",
    "        TokenizerModel tknModel = new TokenizerModel(modelInputStream);\n",
    "        Tokenizer tokenizer = new TokenizerME(tknModel);\n",
    "        \n",
    "        String tokenList[] = tokenizer.tokenize(phrase);\n",
    "        for (String token: tokenList) { System.out.println(token);}\n",
    "    } catch (FileNotFoundException e) {\n",
    "        System.out.println(\"File is not found\");\n",
    "    } catch (IOException e) {\n",
    "        // Handle\n",
    "    }\n",
    "}\n",
    "\n",
    "String sampleText = \"This is the best day indeed!\";\n",
    "tokenizeMaxEntropy(sampleText)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing manually with Scanner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "the\n",
      "best\n",
      "day\n",
      "of\n",
      "my\n",
      "life,\n",
      "as\n",
      "some\n",
      "would\n",
      "say.\n"
     ]
    }
   ],
   "source": [
    "import java.util.ArrayList;\n",
    "import java.util.Scanner;\n",
    "\n",
    "public void tokenizeManually(String phrase){\n",
    "    Scanner scanner = new Scanner(phrase);\n",
    "    ArrayList<String> list = new ArrayList<>();\n",
    "    while (scanner.hasNext()) {\n",
    "        String token = scanner.next();\n",
    "        list.add(token);\n",
    "    }\n",
    "    \n",
    "    for (String token : list) { System.out.println(token); }\n",
    "}\n",
    "\n",
    "String phrase = \"This is the best day of my life, as some would say.\";\n",
    "tokenizeManually(phrase);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training NN tokenizer using specialized text\n",
    "- training text in [`../data/training.train`](../data/training.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.BufferedOutputStream;\n",
    "import java.io.File;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.FileNotFoundException;\n",
    "import java.io.FileOutputStream;\n",
    "import java.io.IOException;\n",
    "import java.io.InputStream;\n",
    "import opennlp.tools.tokenize.TokenSample;\n",
    "import opennlp.tools.tokenize.TokenSampleStream;\n",
    "import opennlp.tools.tokenize.Tokenizer;\n",
    "import opennlp.tools.tokenize.TokenizerFactory;\n",
    "import opennlp.tools.tokenize.TokenizerME;\n",
    "import opennlp.tools.tokenize.TokenizerModel;\n",
    "import opennlp.tools.util.InputStreamFactory;\n",
    "import opennlp.tools.util.ObjectStream;\n",
    "import opennlp.tools.util.PlainTextByLineStream;\n",
    "import opennlp.tools.util.TrainingParameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing events with TwoPass using cutoff of 5\n",
      "\n",
      "\tComputing event counts...  done. 36 events\n",
      "\tIndexing...  done.\n",
      "Sorting and merging events... done. Reduced 36 events to 12.\n",
      "Done indexing in 0.03 s.\n",
      "Incorporating indexed data for training...  \n",
      "done.\n",
      "\tNumber of Event Tokens: 12\n",
      "\t    Number of Outcomes: 2\n",
      "\t  Number of Predicates: 9\n",
      "...done.\n",
      "Computing model parameters ...\n",
      "Performing 100 iterations.\n",
      "  1:  ... loglikelihood=-24.95329850015802\t0.8611111111111112\n",
      "  2:  ... loglikelihood=-14.200654164477221\t0.8611111111111112\n",
      "  3:  ... loglikelihood=-11.526745527757855\t0.8611111111111112\n",
      "  4:  ... loglikelihood=-9.984657035211438\t0.8888888888888888\n",
      "  5:  ... loglikelihood=-8.837634767583115\t0.8888888888888888\n",
      "  6:  ... loglikelihood=-7.925934782768229\t0.8888888888888888\n",
      "  7:  ... loglikelihood=-7.182391009502338\t0.8888888888888888\n",
      "  8:  ... loglikelihood=-6.565411011241649\t0.8888888888888888\n",
      "  9:  ... loglikelihood=-6.045907913839374\t0.9166666666666666\n",
      " 10:  ... loglikelihood=-5.602806368635076\t1.0\n",
      " 11:  ... loglikelihood=-5.220550574471675\t1.0\n",
      " 12:  ... loglikelihood=-4.887470064330368\t1.0\n",
      " 13:  ... loglikelihood=-4.594668511152725\t1.0\n",
      " 14:  ... loglikelihood=-4.33525968046874\t1.0\n",
      " 15:  ... loglikelihood=-4.103836393093284\t1.0\n",
      " 16:  ... loglikelihood=-3.896096538802737\t1.0\n",
      " 17:  ... loglikelihood=-3.7085758164537936\t1.0\n",
      " 18:  ... loglikelihood=-3.5384538681241744\t1.0\n",
      " 19:  ... loglikelihood=-3.3834115872738963\t1.0\n",
      " 20:  ... loglikelihood=-3.2415246277497354\t1.0\n",
      " 21:  ... loglikelihood=-3.1111828911776547\t1.0\n",
      " 22:  ... loglikelihood=-2.9910289158200665\t1.0\n",
      " 23:  ... loglikelihood=-2.8799101986107556\t1.0\n",
      " 24:  ... loglikelihood=-2.7768419146720316\t1.0\n",
      " 25:  ... loglikelihood=-2.680977485222158\t1.0\n",
      " 26:  ... loglikelihood=-2.5915851333562454\t1.0\n",
      " 27:  ... loglikelihood=-2.5080290539640604\t1.0\n",
      " 28:  ... loglikelihood=-2.429754172403413\t1.0\n",
      " 29:  ... loglikelihood=-2.3562737187424276\t1.0\n",
      " 30:  ... loglikelihood=-2.2871590289659873\t1.0\n",
      " 31:  ... loglikelihood=-2.222031121033551\t1.0\n",
      " 32:  ... loglikelihood=-2.160553695588358\t1.0\n",
      " 33:  ... loglikelihood=-2.102427287908607\t1.0\n",
      " 34:  ... loglikelihood=-2.047384356051551\t1.0\n",
      " 35:  ... loglikelihood=-1.9951851348555043\t1.0\n",
      " 36:  ... loglikelihood=-1.9456141199868322\t1.0\n",
      " 37:  ... loglikelihood=-1.8984770730650622\t1.0\n",
      " 38:  ... loglikelihood=-1.8535984599200697\t1.0\n",
      " 39:  ... loglikelihood=-1.8108192506014804\t1.0\n",
      " 40:  ... loglikelihood=-1.7699950228967838\t1.0\n",
      " 41:  ... loglikelihood=-1.7309943215924957\t1.0\n",
      " 42:  ... loglikelihood=-1.693697234116291\t1.0\n",
      " 43:  ... loglikelihood=-1.657994149974138\t1.0\n",
      " 44:  ... loglikelihood=-1.62378467688781\t1.0\n",
      " 45:  ... loglikelihood=-1.5909766910096308\t1.0\n",
      " 46:  ... loglikelihood=-1.5594855022494665\t1.0\n",
      " 47:  ... loglikelihood=-1.5292331187546637\t1.0\n",
      " 48:  ... loglikelihood=-1.5001475970637672\t1.0\n",
      " 49:  ... loglikelihood=-1.4721624665095834\t1.0\n",
      " 50:  ... loglikelihood=-1.4452162181561043\t1.0\n",
      " 51:  ... loglikelihood=-1.4192518499802746\t1.0\n",
      " 52:  ... loglikelihood=-1.3942164612047965\t1.0\n",
      " 53:  ... loglikelihood=-1.3700608896927349\t1.0\n",
      " 54:  ... loglikelihood=-1.3467393871621223\t1.0\n",
      " 55:  ... loglikelihood=-1.3242093276956313\t1.0\n",
      " 56:  ... loglikelihood=-1.302430945628887\t1.0\n",
      " 57:  ... loglikelihood=-1.2813670994189008\t1.0\n",
      " 58:  ... loglikelihood=-1.2609830585361856\t1.0\n",
      " 59:  ... loglikelihood=-1.241246310802594\t1.0\n",
      " 60:  ... loglikelihood=-1.2221263879216397\t1.0\n",
      " 61:  ... loglikelihood=-1.2035947072275701\t1.0\n",
      " 62:  ... loglikelihood=-1.1856244279204868\t1.0\n",
      " 63:  ... loglikelihood=-1.1681903202632264\t1.0\n",
      " 64:  ... loglikelihood=-1.1512686463963577\t1.0\n",
      " 65:  ... loglikelihood=-1.1348370515845536\t1.0\n",
      " 66:  ... loglikelihood=-1.1188744648441893\t1.0\n",
      " 67:  ... loglikelihood=-1.1033610080210645\t1.0\n",
      " 68:  ... loglikelihood=-1.0882779124914836\t1.0\n",
      " 69:  ... loglikelihood=-1.0736074427509488\t1.0\n",
      " 70:  ... loglikelihood=-1.0593328262348916\t1.0\n",
      " 71:  ... loglikelihood=-1.045438188786182\t1.0\n",
      " 72:  ... loglikelihood=-1.0319084952462472\t1.0\n",
      " 73:  ... loglikelihood=-1.0187294947011731\t1.0\n",
      " 74:  ... loglikelihood=-1.0058876699626513\t1.0\n",
      " 75:  ... loglikelihood=-0.9933701909063914\t1.0\n",
      " 76:  ... loglikelihood=-0.9811648713285093\t1.0\n",
      " 77:  ... loglikelihood=-0.9692601290142925\t1.0\n",
      " 78:  ... loglikelihood=-0.9576449487435117\t1.0\n",
      " 79:  ... loglikelihood=-0.946308847983393\t1.0\n",
      " 80:  ... loglikelihood=-0.9352418450440162\t1.0\n",
      " 81:  ... loglikelihood=-0.9244344294924057\t1.0\n",
      " 82:  ... loglikelihood=-0.9138775346404048\t1.0\n",
      " 83:  ... loglikelihood=-0.9035625119387732\t1.0\n",
      " 84:  ... loglikelihood=-0.8934811071250299\t1.0\n",
      " 85:  ... loglikelihood=-0.8836254379865098\t1.0\n",
      " 86:  ... loglikelihood=-0.8739879736123527\t1.0\n",
      " 87:  ... loglikelihood=-0.8645615150194061\t1.0\n",
      " 88:  ... loglikelihood=-0.8553391770469931\t1.0\n",
      " 89:  ... loglikelihood=-0.8463143714245851\t1.0\n",
      " 90:  ... loglikelihood=-0.8374807909247232\t1.0\n",
      " 91:  ... loglikelihood=-0.8288323945207713\t1.0\n",
      " 92:  ... loglikelihood=-0.8203633934759975\t1.0\n",
      " 93:  ... loglikelihood=-0.8120682382964413\t1.0\n",
      " 94:  ... loglikelihood=-0.8039416064856247\t1.0\n",
      " 95:  ... loglikelihood=-0.795978391044187\t1.0\n",
      " 96:  ... loglikelihood=-0.7881736896620514\t1.0\n",
      " 97:  ... loglikelihood=-0.7805227945549726\t1.0\n",
      " 98:  ... loglikelihood=-0.7730211829010772\t1.0\n",
      " 99:  ... loglikelihood=-0.765664507836384\t1.0\n",
      "100:  ... loglikelihood=-0.7584485899716518\t1.0\n"
     ]
    }
   ],
   "source": [
    "// Create input text to train on\n",
    "InputStreamFactory inputStreamFactory = new InputStreamFactory() {\n",
    "    public InputStream createInputStream() throws FileNotFoundException {\n",
    "        return new FileInputStream(\"../data/training-data.train\");\n",
    "    }\n",
    "};\n",
    "\n",
    "try (\n",
    "    ObjectStream<String> stringObjectStream = new PlainTextByLineStream(inputStreamFactory, \"UTF-8\");\n",
    "    ObjectStream<TokenSample> tokenSampleStream = new TokenSampleStream(stringObjectStream);) {\n",
    "    // create model   \n",
    "    TokenizerModel tokenizerModel = TokenizerME.train(tokenSampleStream,\n",
    "                                                      new TokenizerFactory(\"en\", null, true, null), \n",
    "                                                      TrainingParameters.defaultParams());\n",
    "    BufferedOutputStream modelOutputStream = new BufferedOutputStream(new FileOutputStream(\n",
    "                                                    new File(\"../models/mymodel.bin\")));\n",
    "    tokenizerModel.serialize(modelOutputStream);\n",
    "} catch (IOException ex) {\n",
    "    // Handle exception\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In\n",
      "addition\n",
      ",\n",
      "the\n",
      "rook\n",
      "was\n",
      "moved\n",
      "too\n",
      "far\n",
      "to\n",
      "be\n",
      "effective\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "// Test moddel \n",
    "String sampleText = \"In addition, the rook was moved too far to be effective.\";\n",
    "try (InputStream modelInputStream = new FileInputStream(\n",
    "        new File(\"../models\", \"mymodel.bin\"));) {\n",
    "            TokenizerModel tokenizerModel = new TokenizerModel(modelInputStream);\n",
    "            Tokenizer tokenizer = new TokenizerME(tokenizerModel);\n",
    "            String tokenList[] = tokenizer.tokenize(sampleText);\n",
    "            for (String token : tokenList) {\n",
    "                System.out.println(token);\n",
    "            }\n",
    "} catch (FileNotFoundException e) {\n",
    "    // Handle exception\n",
    "} catch (IOException e) {\n",
    "    // Handle exception\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stemming using OpenNLP's `PorterStemmer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The stem of draft is draft\n",
      "The stem of drafted is draft\n",
      "The stem of drafting is draft\n",
      "The stem of drafts is draft\n",
      "The stem of drafty is drafti\n",
      "The stem of draftsman is draftsman\n"
     ]
    }
   ],
   "source": [
    "import opennlp.tools.stemmer.PorterStemmer;\n",
    "\n",
    "String words[] = {\"draft\", \"drafted\", \"drafting\", \"drafts\", \"drafty\", \"draftsman\"};\n",
    "PorterStemmer porterStemmer = new PorterStemmer();\n",
    "for (String word:words) {\n",
    "    String stem = porterStemmer.stem(word);\n",
    "    System.out.println(\"The stem of \" + word + \" is \" + stem);\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Lexical meaning of a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.FileInputStream;\n",
    "import java.io.FileNotFoundException;\n",
    "import java.io.IOException;\n",
    "import java.io.InputStream;\n",
    "import opennlp.tools.lemmatizer.LemmatizerME;\n",
    "import opennlp.tools.lemmatizer.LemmatizerModel;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "CompilationException",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[30m|   \u001b[1m\u001b[30mString[] lemmas = \u001b[0m\u001b[1m\u001b[30m\u001b[41mlemmatizer\u001b[0m\u001b[1m\u001b[30m.lemmatize(tokens, posTags);\u001b[0m",
      "\u001b[1m\u001b[31mcannot find symbol\u001b[0m",
      "\u001b[1m\u001b[31m  symbol:   variable lemmatizer\u001b[0m",
      ""
     ]
    }
   ],
   "source": [
    "LemmatizerModel lemmatizerModel = null;\n",
    "try (InputStream modelInputStream = new FileInputStream(\"../models/en-lemmatizer.bin\")){\n",
    "    lemmatizerModel = new LemmatizerModel(modelInputStream);\n",
    "    LemmatizerME lemmatizer = new LemmatizerME(lemmatizerModel);\n",
    "} catch (FileNotFoundException e) {\n",
    "    // Handle exception\n",
    "} catch (IOException e) {\n",
    "    // Handle exception\n",
    "}\n",
    "\n",
    "String[] tokens = new String[] { \n",
    "    \"The\", \"girls\", \"were\", \"leaving\", \"the\", \n",
    "    \"clubhouse\", \"for\", \"another\", \"adventurous\", \n",
    "    \"afternoon\", \".\" };\n",
    "String[] posTags = new String[] { \"DT\", \"NNS\", \"VBD\", \n",
    "    \"VBG\", \"DT\", \"NN\", \"IN\", \"DT\", \"JJ\", \"NN\", \".\" };\n",
    "String[] lemmas = lemmatizer.lemmatize(tokens, posTags);\n",
    "for (int i = 0; i < tokens.length; i++) {\n",
    "    System.out.println(tokens[i] + \" - \" + lemmas[i]);\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "EvalException",
     "evalue": "Undefined cell magic 'bash'",
     "output_type": "error",
     "traceback": [
      "\u001b[1m\u001b[31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1m\u001b[31mio.github.spencerpark.jupyter.kernel.magic.registry.UndefinedMagicException: Undefined cell magic 'bash'\u001b[0m",
      "\u001b[1m\u001b[31m\tat io.github.spencerpark.jupyter.kernel.magic.registry.Magics.applyCellMagic(Magics.java:34)\u001b[0m",
      "\u001b[1m\u001b[31m\tat io.github.spencerpark.ijava.runtime.Magics.cellMagic(Magics.java:31)\u001b[0m",
      "\u001b[1m\u001b[31m\tat .(#70:1)\u001b[0m"
     ]
    }
   ],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "14.0.1+7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
