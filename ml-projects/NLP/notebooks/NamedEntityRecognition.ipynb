{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "- Using regular expressions to find entities\n",
    "- Using chunks with regular exp to identify entities\n",
    "- Using OpenNLP to find entities in text\n",
    "- Isolating multiple entities types\n",
    "- Using a CRF model to find entities in a document\n",
    "- Using a chunker to find entities\n",
    "- Training a specialized NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nlp@nlpworks.com [29:45]\n",
      "mrnlp@nlpworks.org [74:92]\n"
     ]
    }
   ],
   "source": [
    "import java.util.regex.Matcher;\n",
    "import java.util.regex.Pattern;\n",
    "\n",
    "String sampleText = \"I can normally be reached at nlp@nlpworks.com. \" + \"If not you can email me at mrnlp@nlpworks.org\";\n",
    "String emailRegularExpression = \"[a-zA-Z0-9'._%+-]+@\" + \"(?:[a-zA-Z0-9-]+\\\\.)\" + \"+[a-zA-Z]{2,4}\";\n",
    "Pattern pattern = Pattern.compile(emailRegularExpression);\n",
    "Matcher matcher = pattern.matcher(sampleText);\n",
    "\n",
    "while (matcher.find()) {\n",
    "    System.out.println(matcher.group() + \" [\" + matcher.start() + \":\" + \n",
    "        matcher.end() + \"]\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "888-555-1111 [20:32]\n",
      "55555-4444 [66:76]\n"
     ]
    }
   ],
   "source": [
    "String phoneNumberRegularExpression = \"\\\\d{3}-\\\\d{3}-\\\\d{4}\";\n",
    "String zipCodeRegularExpression = \"[0-9]{5}(\\\\-?[0-9]{4})?\";\n",
    "pattern = Pattern.compile(phoneNumberRegularExpression + \"|\" + \n",
    " zipCodeRegularExpression + \"|\" + emailRegularExpression);\n",
    " sampleText = \"Her phone number is 888-555-1111. You may also need her ZIP code: 55555-4444\";\n",
    "\n",
    "matcher = pattern.matcher(sampleText);\n",
    "while (matcher.find()) {\n",
    "    System.out.println(matcher.group() + \" [\" + matcher.start() + \":\" + \n",
    "        matcher.end() + \"]\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LINGPipe's Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<dependency>\n",
    "    <groupId>de.julielab</groupId>\n",
    "    <artifactId>aliasi-lingpipe</artifactId>\n",
    "    <version>4.1.0</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: hisemail@somecompany.com\tType: EMAIL\n"
     ]
    }
   ],
   "source": [
    "import java.util.Set;\n",
    "import com.aliasi.chunk.Chunk;\n",
    "import com.aliasi.chunk.Chunker;\n",
    "import com.aliasi.chunk.Chunking;\n",
    "import com.aliasi.chunk.RegExChunker;\n",
    "\n",
    "String sampleText = \"His email address is hisemail@somecompany.com.\";\n",
    "String emailRegularExpression = \"[A-Za-z0-9](([_\\\\.\\\\-]?[a-zA-Z0-9]+)*)@(\" + \n",
    "    \"[A-Za-z0-9]+)(([\\\\.\\\\-]?[a-zA-Z0-9]+)*)\\\\.([A-Za-z]{2,})\";\n",
    "\n",
    "Chunker chunker = new RegExChunker(emailRegularExpression,\"EMAIL\",1.0);\n",
    "Chunking chunking = chunker.chunk(sampleText);\n",
    "Set<Chunk> chunkSet = chunking.chunkSet();\n",
    "\n",
    "for (Chunk chunk : chunkSet) {\n",
    "    System.out.println(\"Entity: \" + \n",
    "        sampleText.substring(chunk.start(), chunk.end()) + \n",
    "        \"\\tType: \" + chunk.type());\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenNLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<dependency>\n",
    "    <groupId>org.apache.opennlp</groupId>\n",
    "    <artifactId>opennlp-tools</artifactId>\n",
    "    <version>1.9.0</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.InputStream;\n",
    "import opennlp.tools.namefind.NameFinderME;\n",
    "import opennlp.tools.namefind.TokenNameFinderModel;\n",
    "import opennlp.tools.tokenize.Tokenizer;\n",
    "import opennlp.tools.tokenize.TokenizerME;\n",
    "import opennlp.tools.tokenize.TokenizerModel;\n",
    "import opennlp.tools.util.Span;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity: [1850s] was a date entity found starting at 6 and ending at 7\n",
      "Date: 1850s  Probability: 0.878211895731101\n",
      "Entity: [March] was a date entity found starting at 13 and ending at 15\n",
      "Date: March 3  Probability: 0.9937399307548391\n"
     ]
    }
   ],
   "source": [
    "try (InputStream tokenStream = new FileInputStream(new File(\"../models/en-token.bin\"));\n",
    "        InputStream entityModelInputStream = new FileInputStream(new File(\"../models/en-ner-date.bin\"));) {\n",
    "    TokenizerModel tokenizerModel = new TokenizerModel(tokenStream);\n",
    "    Tokenizer tokenizer = new TokenizerME(tokenizerModel);\n",
    "    TokenNameFinderModel tokenNameFinderModel = new TokenNameFinderModel(entityModelInputStream);\n",
    "    // set class instance\n",
    "    NameFinderME nameFinderME = new NameFinderME(tokenNameFinderModel);\n",
    "\n",
    "    String text = \"The city was founded in the 1850s and its first mayor was born March 3, 1832.\";\n",
    "    String tokens[] = tokenizer.tokenize(text);\n",
    "    Span dateSpans[] = nameFinderME.find(tokens);\n",
    "\n",
    "    for (int i = 0; i < dateSpans.length; i++) {\n",
    "        System.out.print(\"Entity: [\" + tokens[dateSpans[i].getStart()]);\n",
    "        System.out.print(\"] was a \" + dateSpans[i].getType() + \" entity found starting at \" + dateSpans[i].getStart());\n",
    "        System.out.println(\" and ending at \" + dateSpans[i].getEnd());\n",
    "        \n",
    "        // to get actual spans\n",
    "        String date = \"\";\n",
    "        for(int j=dateSpans[i].getStart(); j< dateSpans[i].getEnd(); j++) {\n",
    "            date += tokens[j] + \" \"; \n",
    "        }\n",
    "        // To get probabilities\n",
    "        double[] spanProbs = nameFinderME.probs(dateSpans);\n",
    "        System.out.println(\"Date: \" + date + \" Probability: \" + spanProbs[i]);\n",
    "    }\n",
    "    \n",
    "} catch (Exception ex) {\n",
    " // Handle exception\n",
    "    System.out.println(\"Could not find model files\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Identifying multiple entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.InputStream;\n",
    "import java.util.ArrayList;\n",
    "import opennlp.tools.namefind.NameFinderME;\n",
    "import opennlp.tools.namefind.TokenNameFinderModel;\n",
    "import opennlp.tools.tokenize.Tokenizer;\n",
    "import opennlp.tools.tokenize.TokenizerME;\n",
    "import opennlp.tools.tokenize.TokenizerModel;\n",
    "import opennlp.tools.util.Span;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "String sentences[] = { \n",
    "    \"Sam and Mary left on Friday, November 12. \",\n",
    "    \"They stopped in Boston at an ATM to get $300 for expenses. \",\n",
    "    \"While they were there Sam bumped into an old friend who was on his way to work at ATT. \",\n",
    "    \"They decided to leave together and departed for Maine\" };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "\tEntity: Sam - Entity Type: person\n",
      "\tEntity: Mary - Entity Type: person\n",
      "Sentence 2\n",
      "\tEntity: Boston - Entity Type: location\n",
      "\tEntity: $300 - Entity Type: money\n",
      "Sentence 3\n",
      "Sentence 4\n",
      "\tEntity: Maine - Entity Type: location\n"
     ]
    }
   ],
   "source": [
    "try (InputStream tokenStream = new FileInputStream(new File(\"../models/en-token.bin\"))) {\n",
    "    TokenizerModel tokenModel = new TokenizerModel(tokenStream);\n",
    "    \n",
    "    Tokenizer tokenizer = new TokenizerME(tokenModel);\n",
    "    String modelNames[] = { \n",
    "        \"../models/en-ner-person.bin\", \"../models/en-ner-location.bin\", \n",
    "        \"../models/en-ner-organization.bin\", \"../models/en-ner-money.bin\", \n",
    "        \"../models/en-ner-time.bin\" \n",
    "    };\n",
    "    for (int i = 0; i < sentences.length; i++) {\n",
    "        System.out.println(\"Sentence \" + (i + 1));\n",
    "        for (String name : modelNames) {\n",
    "            TokenNameFinderModel entityModel = new TokenNameFinderModel(new FileInputStream(new File(name)));\n",
    "            NameFinderME nameFinderME = new NameFinderME(entityModel);\n",
    "            \n",
    "            // process sentence\n",
    "            String tokens[] = tokenizer.tokenize(sentences[i]);\n",
    "            Span spans[] = nameFinderME.find(tokens);\n",
    "            \n",
    "            // find location of entities\n",
    "            for (Span span : spans) {\n",
    "                System.out.print(\"\\tEntity: \");\n",
    "                for (int j = span.getStart(); j < span.getEnd(); j++) {\n",
    "                    System.out.print(tokens[j]);\n",
    "                }\n",
    "                System.out.println(\" - Entity Type: \" + span.getType());\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "} catch (Exception ex) {\n",
    "// Handle exceptions\n",
    "    System.out.println(\"Cant find model files\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using a LINGPipe's Chunker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<dependency>\n",
    "    <groupId>de.julielab</groupId>\n",
    "    <artifactId>aliasi-lingpipe</artifactId>\n",
    "    <version>4.1.0</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import java.util.Set;\n",
    "import com.aliasi.chunk.Chunk;\n",
    "import com.aliasi.chunk.Chunker;\n",
    "import com.aliasi.chunk.Chunking;\n",
    "import com.aliasi.util.AbstractExternalizable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "String sentences[] = { \"Sam and Mary left on Friday, November 12. \",\n",
    "    \"They stopped in Boston at an ATM to get $300 for expenses. \",\n",
    "    \"While they were there Sam bumped into an old friend, Mr. Smith, \" +\n",
    "    \"who was on his way to work at ATT. \",\n",
    "    \"They decided to leave together and departed for Maine\" };\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "can't find models\n"
     ]
    }
   ],
   "source": [
    "try {\n",
    "    File modelFile = new File(\"./models/ne-en-news-muc6.AbstractCharLmRescoringChunker\");\n",
    "    Chunker chunker = (Chunker) AbstractExternalizable.readObject(modelFile);\n",
    "    for (int index = 0; index < sentences.length; index++) {\n",
    "        System.out.println(\"Sentence \" + (index + 1));\n",
    "        Chunking chunking = chunker.chunk(sentences[index]);\n",
    "        Set<Chunk> set = chunking.chunkSet();\n",
    "        for (Chunk chunk : set) {\n",
    "            System.out.println(\"\\tEntity: \" + sentences[index].substring(chunk.start(), chunk.end()) + \n",
    "            \"\\tType: \"+ chunk.type());\n",
    "        }\n",
    "    }\n",
    "} catch (IOException | ClassNotFoundException ex) {\n",
    " // Handle exception\n",
    "    System.out.println(\"can't find models\");\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training specialized NER "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<dependency>\n",
    "    <groupId>org.apache.opennlp</groupId>\n",
    "    <artifactId>opennlp-tools</artifactId>\n",
    "    <version>1.9.0</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.BufferedOutputStream;\n",
    "import java.io.File;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.FileOutputStream;\n",
    "import java.io.IOException;\n",
    "import java.io.InputStream;\n",
    "import java.io.OutputStream;\n",
    "import opennlp.tools.namefind.NameFinderME;\n",
    "import opennlp.tools.namefind.NameSample;\n",
    "import opennlp.tools.namefind.NameSampleDataStream;\n",
    "import opennlp.tools.namefind.TokenNameFinderFactory;\n",
    "import opennlp.tools.namefind.TokenNameFinderModel;\n",
    "import opennlp.tools.tokenize.Tokenizer;\n",
    "import opennlp.tools.tokenize.TokenizerME;\n",
    "import opennlp.tools.tokenize.TokenizerModel;\n",
    "import opennlp.tools.util.InputStreamFactory;\n",
    "import opennlp.tools.util.ObjectStream;\n",
    "import opennlp.tools.util.PlainTextByLineStream;\n",
    "import opennlp.tools.util.Span;\n",
    "import opennlp.tools.util.TrainingParameters;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing events with TwoPass using cutoff of 5\n",
      "\n",
      "\tComputing event counts...  done. 41 events\n",
      "\tIndexing...  done.\n",
      "Sorting and merging events... done. Reduced 41 events to 32.\n",
      "Done indexing in 0.01 s.\n",
      "Incorporating indexed data for training...  \n",
      "done.\n",
      "\tNumber of Event Tokens: 32\n",
      "\t    Number of Outcomes: 2\n",
      "\t  Number of Predicates: 26\n",
      "...done.\n",
      "Computing model parameters ...\n",
      "Performing 100 iterations.\n",
      "  1:  ... loglikelihood=-28.41903440295774\t0.8292682926829268\n",
      "  2:  ... loglikelihood=-18.658742208955097\t0.8292682926829268\n",
      "  3:  ... loglikelihood=-15.62851980531423\t0.8292682926829268\n",
      "  4:  ... loglikelihood=-13.750875675138143\t0.8292682926829268\n",
      "  5:  ... loglikelihood=-12.460491980390133\t0.8536585365853658\n",
      "  6:  ... loglikelihood=-11.528373364695357\t0.8536585365853658\n",
      "  7:  ... loglikelihood=-10.826209400783736\t0.926829268292683\n",
      "  8:  ... loglikelihood=-10.278176569510599\t0.926829268292683\n",
      "  9:  ... loglikelihood=-9.83757840876244\t0.926829268292683\n",
      " 10:  ... loglikelihood=-9.474464725485133\t0.926829268292683\n",
      " 11:  ... loglikelihood=-9.168904530047493\t0.926829268292683\n",
      " 12:  ... loglikelihood=-8.907196378947637\t0.926829268292683\n",
      " 13:  ... loglikelihood=-8.679649299057852\t0.926829268292683\n",
      " 14:  ... loglikelihood=-8.47923525873824\t0.9024390243902439\n",
      " 15:  ... loglikelihood=-8.300743548847333\t0.9024390243902439\n",
      " 16:  ... loglikelihood=-8.140234298167723\t0.9024390243902439\n",
      " 17:  ... loglikelihood=-7.994675893977279\t0.9024390243902439\n",
      " 18:  ... loglikelihood=-7.861698671534931\t0.9024390243902439\n",
      " 19:  ... loglikelihood=-7.739423987454773\t0.9024390243902439\n",
      " 20:  ... loglikelihood=-7.626343295775151\t0.9024390243902439\n",
      " 21:  ... loglikelihood=-7.521231084997725\t0.9024390243902439\n",
      " 22:  ... loglikelihood=-7.4230811822252525\t0.9024390243902439\n",
      " 23:  ... loglikelihood=-7.33105946384261\t0.9024390243902439\n",
      " 24:  ... loglikelihood=-7.244468270009663\t0.9024390243902439\n",
      " 25:  ... loglikelihood=-7.162719291372642\t0.9024390243902439\n",
      " 26:  ... loglikelihood=-7.085312672302853\t0.9024390243902439\n",
      " 27:  ... loglikelihood=-7.011820733172927\t0.9024390243902439\n",
      " 28:  ... loglikelihood=-6.941875164983221\t0.9024390243902439\n",
      " 29:  ... loglikelihood=-6.875156862849618\t0.9024390243902439\n",
      " 30:  ... loglikelihood=-6.811387785379667\t0.9024390243902439\n",
      " 31:  ... loglikelihood=-6.7503243841698115\t0.9024390243902439\n",
      " 32:  ... loglikelihood=-6.691752261043739\t0.9024390243902439\n",
      " 33:  ... loglikelihood=-6.63548179333541\t0.9024390243902439\n",
      " 34:  ... loglikelihood=-6.581344528439046\t0.9024390243902439\n",
      " 35:  ... loglikelihood=-6.5291901941695425\t0.9024390243902439\n",
      " 36:  ... loglikelihood=-6.478884205504978\t0.9024390243902439\n",
      " 37:  ... loglikelihood=-6.430305574054352\t0.9024390243902439\n",
      " 38:  ... loglikelihood=-6.383345146273233\t0.9024390243902439\n",
      " 39:  ... loglikelihood=-6.337904111594623\t0.9024390243902439\n",
      " 40:  ... loglikelihood=-6.293892733384076\t0.9024390243902439\n",
      " 41:  ... loglikelihood=-6.2512292647951595\t0.9024390243902439\n",
      " 42:  ... loglikelihood=-6.209839018806576\t0.9024390243902439\n",
      " 43:  ... loglikelihood=-6.169653567421315\t0.9024390243902439\n",
      " 44:  ... loglikelihood=-6.130610049543523\t0.9024390243902439\n",
      " 45:  ... loglikelihood=-6.0926505706785195\t0.9024390243902439\n",
      " 46:  ... loglikelihood=-6.055721680522645\t0.9024390243902439\n",
      " 47:  ... loglikelihood=-6.019773916872783\t0.9024390243902439\n",
      " 48:  ... loglikelihood=-5.98476140620683\t0.9024390243902439\n",
      " 49:  ... loglikelihood=-5.950641512855911\t0.9024390243902439\n",
      " 50:  ... loglikelihood=-5.917374529977325\t0.9024390243902439\n",
      " 51:  ... loglikelihood=-5.884923406598657\t0.9024390243902439\n",
      " 52:  ... loglikelihood=-5.853253505882295\t0.9024390243902439\n",
      " 53:  ... loglikelihood=-5.8223323904895405\t0.9024390243902439\n",
      " 54:  ... loglikelihood=-5.792129631532497\t0.9024390243902439\n",
      " 55:  ... loglikelihood=-5.762616638111516\t0.926829268292683\n",
      " 56:  ... loglikelihood=-5.733766504864285\t0.926829268292683\n",
      " 57:  ... loglikelihood=-5.705553875313398\t0.926829268292683\n",
      " 58:  ... loglikelihood=-5.677954819104328\t0.926829268292683\n",
      " 59:  ... loglikelihood=-5.650946721484507\t0.926829268292683\n",
      " 60:  ... loglikelihood=-5.624508183594219\t0.926829268292683\n",
      " 61:  ... loglikelihood=-5.598618932327742\t0.926829268292683\n",
      " 62:  ... loglikelihood=-5.573259738683666\t0.926829268292683\n",
      " 63:  ... loglikelihood=-5.548412343660975\t0.926829268292683\n",
      " 64:  ... loglikelihood=-5.524059390875785\t0.926829268292683\n",
      " 65:  ... loglikelihood=-5.5001843651755875\t0.926829268292683\n",
      " 66:  ... loglikelihood=-5.476771536615844\t0.926829268292683\n",
      " 67:  ... loglikelihood=-5.453805909240133\t0.926829268292683\n",
      " 68:  ... loglikelihood=-5.431273174170932\t0.926829268292683\n",
      " 69:  ... loglikelihood=-5.409159666575863\t0.926829268292683\n",
      " 70:  ... loglikelihood=-5.387452326124063\t0.926829268292683\n",
      " 71:  ... loglikelihood=-5.366138660591166\t0.926829268292683\n",
      " 72:  ... loglikelihood=-5.34520671230958\t0.926829268292683\n",
      " 73:  ... loglikelihood=-5.324645027194113\t0.926829268292683\n",
      " 74:  ... loglikelihood=-5.304442626102548\t0.926829268292683\n",
      " 75:  ... loglikelihood=-5.284588978316432\t0.926829268292683\n",
      " 76:  ... loglikelihood=-5.265073976950185\t0.926829268292683\n",
      " 77:  ... loglikelihood=-5.245887916116645\t0.926829268292683\n",
      " 78:  ... loglikelihood=-5.227021469694991\t0.926829268292683\n",
      " 79:  ... loglikelihood=-5.208465671562508\t0.926829268292683\n",
      " 80:  ... loglikelihood=-5.190211897165724\t0.926829268292683\n",
      " 81:  ... loglikelihood=-5.172251846318732\t0.926829268292683\n",
      " 82:  ... loglikelihood=-5.154577527127456\t0.926829268292683\n",
      " 83:  ... loglikelihood=-5.137181240948426\t0.926829268292683\n",
      " 84:  ... loglikelihood=-5.12005556829937\t0.926829268292683\n",
      " 85:  ... loglikelihood=-5.1031933556466145\t0.926829268292683\n",
      " 86:  ... loglikelihood=-5.086587703001348\t0.926829268292683\n",
      " 87:  ... loglikelihood=-5.0702319522629695\t0.926829268292683\n",
      " 88:  ... loglikelihood=-5.054119676253299\t0.9512195121951219\n",
      " 89:  ... loglikelihood=-5.038244668390548\t0.9512195121951219\n",
      " 90:  ... loglikelihood=-5.022600932956317\t0.9512195121951219\n",
      " 91:  ... loglikelihood=-5.007182675913029\t0.9512195121951219\n",
      " 92:  ... loglikelihood=-4.991984296232866\t0.9512195121951219\n",
      " 93:  ... loglikelihood=-4.977000377702487\t0.9512195121951219\n",
      " 94:  ... loglikelihood=-4.962225681170897\t0.9512195121951219\n",
      " 95:  ... loglikelihood=-4.947655137210434\t0.9512195121951219\n",
      " 96:  ... loglikelihood=-4.933283839163348\t0.9512195121951219\n",
      " 97:  ... loglikelihood=-4.91910703654861\t0.9512195121951219\n",
      " 98:  ... loglikelihood=-4.905120128805628\t0.9512195121951219\n",
      " 99:  ... loglikelihood=-4.891318659353344\t0.9512195121951219\n",
      "100:  ... loglikelihood=-4.877698309944895\t0.9512195121951219\n"
     ]
    }
   ],
   "source": [
    "InputStreamFactory inputStreamFactory = new InputStreamFactory() {\n",
    "    public InputStream createInputStream() throws IOException {\n",
    "        return new FileInputStream(\"../data/training-data2.train\");\n",
    "    }\n",
    "};\n",
    "\n",
    "try (OutputStream modelOutputStream = new BufferedOutputStream(new FileOutputStream(new File(\"location-model.bin\")));\n",
    "        ObjectStream<String> stringStream = new PlainTextByLineStream(inputStreamFactory, \"UTF-8\");\n",
    "        ObjectStream<NameSample> nameSampleStream = new NameSampleDataStream(stringStream);) {\n",
    "            TokenNameFinderModel locationModel = NameFinderME.train(\"en\", \"LOCATION\",\n",
    "                                                                    nameSampleStream,\n",
    "                                                                    TrainingParameters.defaultParams(), \n",
    "                                                                    new TokenNameFinderFactory());\n",
    "            locationModel.serialize(modelOutputStream);\n",
    "} catch (IOException ex) {\n",
    " // Handle exceptions\n",
    "    System.out.println(\"Can't find models\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "try (InputStream tokenInputStream = new FileInputStream(new File(\"../models/en-token.bin\"));\n",
    "        InputStream entityModelInputStream = new FileInputStream(new File(\"../models/location-model.bin\"));) {\n",
    "            TokenizerModel tokenizerModel = new TokenizerModel(tokenInputStream);\n",
    "            Tokenizer tokenizer = new TokenizerME(tokenizerModel);\n",
    "            TokenNameFinderModel tokenNameFinderModel = new TokenNameFinderModel(entityModelInputStream);\n",
    "            NameFinderME nameFinderME = new NameFinderME(tokenNameFinderModel);\n",
    "            String text = \"The city of Cairo is quite large. However, Quebec is not quite as big.\";\n",
    " \n",
    "            String tokens[] = tokenizer.tokenize(text);\n",
    "            Span locationSpans[] = nameFinderME.find(tokens);\n",
    "            for (int i = 0; i < locationSpans.length; i++) {\n",
    "                System.out.println(\"Entity: [\" + \n",
    "                    tokens[locationSpans[i].getStart()] + \"]\");\n",
    "            }\n",
    "} catch (Exception ex) {\n",
    " // Handle exceptions\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "14.0.1+7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
