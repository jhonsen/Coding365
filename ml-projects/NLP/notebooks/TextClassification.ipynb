{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "- Training a maximum entropy model for text classification\n",
    "- Classifying documents using a maximum entropy model\n",
    "- Classifying documents using the Stanford API\n",
    "- Training a model to classify text using LingPipe\n",
    "- Using LingPipe to classify text\n",
    "- Detecting spam\n",
    "- Performing sentiment analysis on reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<dependency>\n",
    "    <groupId>org.apache.opennlp</groupId>\n",
    "    <artifactId>opennlp-tools</artifactId>\n",
    "    <version>1.5.3</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.BufferedOutputStream;\n",
    "import java.io.File;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.FileNotFoundException;\n",
    "import java.io.FileOutputStream;\n",
    "import java.io.IOException;\n",
    "import java.io.InputStream;\n",
    "import java.io.OutputStream;\n",
    "import java.nio.charset.StandardCharsets;\n",
    "import opennlp.tools.doccat.DoccatModel;\n",
    "import opennlp.tools.doccat.DocumentCategorizerME;\n",
    "import opennlp.tools.doccat.DocumentCategorizerEvaluator;\n",
    "import opennlp.tools.doccat.DocumentSample;\n",
    "import opennlp.tools.doccat.DocumentSampleStream;\n",
    "import opennlp.tools.util.ObjectStream;\n",
    "import opennlp.tools.util.PlainTextByLineStream;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing events using cutoff of 5\n",
      "\n",
      "\tComputing event counts...  done. 10 events\n",
      "\tIndexing...  done.\n",
      "Sorting and merging events... done. Reduced 10 events to 10.\n",
      "Done indexing.\n",
      "Incorporating indexed data for training...  \n",
      "done.\n",
      "\tNumber of Event Tokens: 10\n",
      "\t    Number of Outcomes: 2\n",
      "\t  Number of Predicates: 16\n",
      "...done.\n",
      "Computing model parameters ...\n",
      "Performing 100 iterations.\n",
      "  1:  ... loglikelihood=-6.931471805599453\t0.5\n",
      "  2:  ... loglikelihood=-5.8239977220870305\t1.0\n",
      "  3:  ... loglikelihood=-5.040052010151242\t1.0\n",
      "  4:  ... loglikelihood=-4.462637719223578\t1.0\n",
      "  5:  ... loglikelihood=-4.021008206988369\t1.0\n",
      "  6:  ... loglikelihood=-3.6716177499083407\t1.0\n",
      "  7:  ... loglikelihood=-3.3871687442184633\t1.0\n",
      "  8:  ... loglikelihood=-3.1500333330426806\t1.0\n",
      "  9:  ... loglikelihood=-2.948441846322817\t1.0\n",
      " 10:  ... loglikelihood=-2.7742780498825166\t1.0\n",
      " 11:  ... loglikelihood=-2.6217774354777066\t1.0\n",
      " 12:  ... loglikelihood=-2.486736217459275\t1.0\n",
      " 13:  ... loglikelihood=-2.3660158578319015\t1.0\n",
      " 14:  ... loglikelihood=-2.2572235522844273\t1.0\n",
      " 15:  ... loglikelihood=-2.1585005894092504\t1.0\n",
      " 16:  ... loglikelihood=-2.068378707719667\t1.0\n",
      " 17:  ... loglikelihood=-1.985680431946904\t1.0\n",
      " 18:  ... loglikelihood=-1.9094485299385593\t1.0\n",
      " 19:  ... loglikelihood=-1.838895167756793\t1.0\n",
      " 20:  ... loglikelihood=-1.7733646507220715\t1.0\n",
      " 21:  ... loglikelihood=-1.7123057023089139\t1.0\n",
      " 22:  ... loglikelihood=-1.655250548562612\t1.0\n",
      " 23:  ... loglikelihood=-1.6017989315031747\t1.0\n",
      " 24:  ... loglikelihood=-1.5516057420135616\t1.0\n",
      " 25:  ... loglikelihood=-1.5043713448620364\t1.0\n",
      " 26:  ... loglikelihood=-1.4598339301359313\t1.0\n",
      " 27:  ... loglikelihood=-1.4177634070960166\t1.0\n",
      " 28:  ... loglikelihood=-1.3779564844056968\t1.0\n",
      " 29:  ... loglikelihood=-1.3402326719019797\t1.0\n",
      " 30:  ... loglikelihood=-1.3044310048660428\t1.0\n",
      " 31:  ... loglikelihood=-1.270407339728444\t1.0\n",
      " 32:  ... loglikelihood=-1.2380321054928396\t1.0\n",
      " 33:  ... loglikelihood=-1.2071884214605901\t1.0\n",
      " 34:  ... loglikelihood=-1.1777705115842172\t1.0\n",
      " 35:  ... loglikelihood=-1.1496823607326967\t1.0\n",
      " 36:  ... loglikelihood=-1.1228365695721934\t1.0\n",
      " 37:  ... loglikelihood=-1.0971533735562373\t1.0\n",
      " 38:  ... loglikelihood=-1.0725597983362336\t1.0\n",
      " 39:  ... loglikelihood=-1.0489889292276702\t1.0\n",
      " 40:  ... loglikelihood=-1.0263792765547892\t1.0\n",
      " 41:  ... loglikelihood=-1.0046742220114124\t1.0\n",
      " 42:  ... loglikelihood=-0.9838215338163709\t1.0\n",
      " 43:  ... loglikelihood=-0.9637729405585598\t1.0\n",
      " 44:  ... loglikelihood=-0.9444837553328362\t1.0\n",
      " 45:  ... loglikelihood=-0.925912543151189\t1.0\n",
      " 46:  ... loglikelihood=-0.9080208257409711\t1.0\n",
      " 47:  ... loglikelihood=-0.8907728187655186\t1.0\n",
      " 48:  ... loglikelihood=-0.8741351972629315\t1.0\n",
      " 49:  ... loglikelihood=-0.8580768857278238\t1.0\n",
      " 50:  ... loglikelihood=-0.8425688697836601\t1.0\n",
      " 51:  ... loglikelihood=-0.8275840268297128\t1.0\n",
      " 52:  ... loglikelihood=-0.8130969734124884\t1.0\n",
      " 53:  ... loglikelihood=-0.7990839273794419\t1.0\n",
      " 54:  ... loglikelihood=-0.7855225831329994\t1.0\n",
      " 55:  ... loglikelihood=-0.7723919985236345\t1.0\n",
      " 56:  ... loglikelihood=-0.7596724921086491\t1.0\n",
      " 57:  ... loglikelihood=-0.7473455496638342\t1.0\n",
      " 58:  ... loglikelihood=-0.735393738972745\t1.0\n",
      " 59:  ... loglikelihood=-0.7238006320366588\t1.0\n",
      " 60:  ... loglikelihood=-0.7125507339502855\t1.0\n",
      " 61:  ... loglikelihood=-0.7016294177766088\t1.0\n",
      " 62:  ... loglikelihood=-0.6910228648307859\t1.0\n",
      " 63:  ... loglikelihood=-0.6807180098496589\t1.0\n",
      " 64:  ... loglikelihood=-0.6707024905815298\t1.0\n",
      " 65:  ... loglikelihood=-0.6609646013816655\t1.0\n",
      " 66:  ... loglikelihood=-0.6514932504434992\t1.0\n",
      " 67:  ... loglikelihood=-0.6422779203346519\t1.0\n",
      " 68:  ... loglikelihood=-0.6333086315413107\t1.0\n",
      " 69:  ... loglikelihood=-0.624575908754921\t1.0\n",
      " 70:  ... loglikelihood=-0.6160707496620066\t1.0\n",
      " 71:  ... loglikelihood=-0.6077845960217699\t1.0\n",
      " 72:  ... loglikelihood=-0.5997093068372301\t1.0\n",
      " 73:  ... loglikelihood=-0.5918371334444946\t1.0\n",
      " 74:  ... loglikelihood=-0.5841606963614776\t1.0\n",
      " 75:  ... loglikelihood=-0.5766729637523577\t1.0\n",
      " 76:  ... loglikelihood=-0.5693672313774011\t1.0\n",
      " 77:  ... loglikelihood=-0.562237103909765\t1.0\n",
      " 78:  ... loglikelihood=-0.5552764775116275\t1.0\n",
      " 79:  ... loglikelihood=-0.5484795235716122\t1.0\n",
      " 80:  ... loglikelihood=-0.5418406735141543\t1.0\n",
      " 81:  ... loglikelihood=-0.5353546045992754\t1.0\n",
      " 82:  ... loglikelihood=-0.5290162266382529\t1.0\n",
      " 83:  ... loglikelihood=-0.5228206695570472\t1.0\n",
      " 84:  ... loglikelihood=-0.516763271745102\t1.0\n",
      " 85:  ... loglikelihood=-0.5108395691323245\t1.0\n",
      " 86:  ... loglikelihood=-0.505045284941798\t1.0\n",
      " 87:  ... loglikelihood=-0.49937632007003485\t1.0\n",
      " 88:  ... loglikelihood=-0.4938287440504932\t1.0\n",
      " 89:  ... loglikelihood=-0.48839878655961677\t1.0\n",
      " 90:  ... loglikelihood=-0.48308282942786\t1.0\n",
      " 91:  ... loglikelihood=-0.477877399121137\t1.0\n",
      " 92:  ... loglikelihood=-0.4727791596607778\t1.0\n",
      " 93:  ... loglikelihood=-0.46778490595254396\t1.0\n",
      " 94:  ... loglikelihood=-0.4628915574974931\t1.0\n",
      " 95:  ... loglikelihood=-0.4580961524595205\t1.0\n",
      " 96:  ... loglikelihood=-0.4533958420662958\t1.0\n",
      " 97:  ... loglikelihood=-0.44878788532201885\t1.0\n",
      " 98:  ... loglikelihood=-0.44426964401200864\t1.0\n",
      " 99:  ... loglikelihood=-0.43983857798058357\t1.0\n",
      "100:  ... loglikelihood=-0.4354922406650239\t1.0\n"
     ]
    }
   ],
   "source": [
    "try (InputStream dataInputStream = new FileInputStream(\"../data/en-frograt.train\")) {\n",
    "    // Create input stream for training data\n",
    "    ObjectStream<String> objectStream = new PlainTextByLineStream(dataInputStream, StandardCharsets.UTF_8);\n",
    "    ObjectStream<DocumentSample> documentSampleStream = new DocumentSampleStream(objectStream);\n",
    "    // train the model\n",
    "    DoccatModel documentCategorizationModel = DocumentCategorizerME.train(\"en\", documentSampleStream);\n",
    "    OutputStream modelOutputStream = new BufferedOutputStream(new FileOutputStream(new File(\"../models/en-frograt.bin\")));\n",
    "    // Serialize the model\n",
    "    OutputStream modelBufferedOutputStream = new BufferedOutputStream(modelOutputStream);\n",
    "    documentCategorizationModel.serialize(modelBufferedOutputStream);\n",
    "    \n",
    "} catch (FileNotFoundException e) {\n",
    "    // Handle exceptions\n",
    "    System.out.println(\"Can't find files!\");\n",
    "} catch (IOException e) {\n",
    "    // Handle exceptions\n",
    "    System.out.println(\"Something off here!\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------\n",
      "Category : Probability\n",
      "---------------------------------\n",
      "frog : 0.5797537229352753\n",
      "rat : 0.42024627706472467\n",
      "---------------------------------\n",
      "\n",
      "frog : is the predicted category for the given sentence.\n"
     ]
    }
   ],
   "source": [
    "// Testing the model\n",
    "\n",
    "try (InputStream modelInputStream = new FileInputStream(\"../models/en-frograt.bin\")) {\n",
    "    // Create input stream for training data\n",
    "    DoccatModel model = new DoccatModel(modelInputStream);\n",
    "    DocumentCategorizerME myCategorizer = new DocumentCategorizerME(model);\n",
    "    String[] docWords = \"This amphibious animal makes a ribbidty sound. It also lives in both water and land. It's cold blooded one.\".replaceAll(\"[^A-Za-z]\", \" \").split(\" \");\n",
    "    double[] aProbs = myCategorizer.categorize(docWords);\n",
    "    String predictedCategory = myCategorizer.getBestCategory(aProbs);\n",
    "    \n",
    "    System.out.println(\"\\n---------------------------------\\nCategory : Probability\\n---------------------------------\");\n",
    "    for(int i=0; i<myCategorizer.getNumberOfCategories(); i++){\n",
    "        System.out.println(myCategorizer.getCategory(i) + \" : \"+ aProbs[i]);\n",
    "    }\n",
    "    System.out.println(\"---------------------------------\");\n",
    "\n",
    "    System.out.println(\"\\n\"+ predictedCategory +\" : is the predicted category for the given sentence.\");\n",
    "\n",
    "} catch (FileNotFoundException e) {\n",
    "    // Handle exceptions\n",
    "    System.out.println(\"Can't find files!\");\n",
    "} catch (IOException e) {\n",
    "    // Handle exceptions\n",
    "    System.out.println(\"Something off here!\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sample: 0\n",
      "Sentence  : This amphibious animal makes a ribbidty sound. It also lives in both water and land. It's cold blooded one.\n",
      "Predicted : frog\n",
      "Accuracy  : 1.0\n",
      "For sample: 1\n",
      "Sentence  : The fur of the rodent is very smooth and white. It nurses its pups for 21 days until it continues to live.\n",
      "Predicted : rat\n",
      "Accuracy  : 1.0\n"
     ]
    }
   ],
   "source": [
    "//\n",
    "try (InputStream modelInputStream = new FileInputStream(\"../models/en-frograt.bin\")){\n",
    "    DoccatModel model = new DoccatModel(modelInputStream);\n",
    "    DocumentCategorizerME myCategorizer = new DocumentCategorizerME(model);\n",
    "    \n",
    "    DocumentCategorizerEvaluator evaluator = new DocumentCategorizerEvaluator(myCategorizer);\n",
    "    \n",
    "    String category[] = {\"frog\",\"rat\"};\n",
    "    String content[] = {\"This amphibious animal makes a ribbidty sound. It also lives in both water and land. It's cold blooded one.\",\n",
    "                        \"The fur of the rodent is very smooth and white. It nurses its pups for 21 days until it continues to live.\"};\n",
    "    for (int i=0; i<category.length; i++) {\n",
    "        DocumentSample sample = new DocumentSample(category[i], content[i]);\n",
    "        evaluator.evaluteSample(sample);\n",
    "        double result = evaluator.getAccuracy();\n",
    "        System.out.println(\"For sample: \" + i );\n",
    "        System.out.println(\"Sentence  : \" + content[i]);\n",
    "        System.out.println(\"Predicted : \" + category[i]);    \n",
    "        System.out.println(\"Accuracy  : \" + result);\n",
    "    }\n",
    "    \n",
    "} catch (FileNotFoundException e) {\n",
    "    System.out.println(\"Can't find model files\");\n",
    "} catch (IOException e) {\n",
    "    System.out.println(\"Something off here!\");\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load serialized trained model\n",
    "import java.util.Scanner;\n",
    "import opennlp.tools.tokenize.TokenizerME;\n",
    "import opennlp.tools.tokenize.TokenizerModel;\n",
    "\n",
    "private String[] getTokens(String sentence) {\n",
    "\n",
    "    // Use model that was created in earlier tokenizer tutorial\n",
    "    try (InputStream modelIn = new FileInputStream(\"../models/en-token.bin\")) {\n",
    "\n",
    "        TokenizerME myCategorizer = new TokenizerME(new TokenizerModel(modelIn));\n",
    "\n",
    "        String[] tokens = myCategorizer.tokenize(sentence);\n",
    "\n",
    "        for (String t : tokens) {\n",
    "            System.out.println(\"Tokens: \" + t);\n",
    "        }\n",
    "        return tokens;\n",
    "\n",
    "    } catch (Exception e) {\n",
    "        e.printStackTrace();\n",
    "    }\n",
    "    return null;\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "14.0.1+7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
