{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Classification\n",
    "- Training a maximum entropy model for text classification\n",
    "- Classifying documents using a maximum entropy model\n",
    "- Classifying documents using the Stanford API\n",
    "- Training a model to classify text using LingPipe\n",
    "- Using LingPipe to classify text\n",
    "- Detecting spam\n",
    "- Performing sentiment analysis on reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<dependency>\n",
    "    <groupId>org.apache.opennlp</groupId>\n",
    "    <artifactId>opennlp-tools</artifactId>\n",
    "    <version>1.5.3</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.BufferedOutputStream;\n",
    "import java.io.File;\n",
    "import java.io.FileInputStream;\n",
    "import java.io.FileNotFoundException;\n",
    "import java.io.FileOutputStream;\n",
    "import java.io.IOException;\n",
    "import java.io.InputStream;\n",
    "import java.io.OutputStream;\n",
    "import java.nio.charset.StandardCharsets;\n",
    "import opennlp.tools.doccat.DoccatModel;\n",
    "import opennlp.tools.doccat.DocumentCategorizerME;\n",
    "import opennlp.tools.doccat.DocumentCategorizerEvaluator;\n",
    "import opennlp.tools.doccat.DocumentSample;\n",
    "import opennlp.tools.doccat.DocumentSampleStream;\n",
    "import opennlp.tools.util.ObjectStream;\n",
    "import opennlp.tools.util.PlainTextByLineStream;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing events using cutoff of 5\n",
      "\n",
      "\tComputing event counts...  done. 10 events\n",
      "\tIndexing...  done.\n",
      "Sorting and merging events... done. Reduced 10 events to 10.\n",
      "Done indexing.\n",
      "Incorporating indexed data for training...  \n",
      "done.\n",
      "\tNumber of Event Tokens: 10\n",
      "\t    Number of Outcomes: 2\n",
      "\t  Number of Predicates: 16\n",
      "...done.\n",
      "Computing model parameters ...\n",
      "Performing 100 iterations.\n",
      "  1:  ... loglikelihood=-6.931471805599453\t0.5\n",
      "  2:  ... loglikelihood=-5.8239977220870305\t1.0\n",
      "  3:  ... loglikelihood=-5.040052010151242\t1.0\n",
      "  4:  ... loglikelihood=-4.462637719223578\t1.0\n",
      "  5:  ... loglikelihood=-4.021008206988369\t1.0\n",
      "  6:  ... loglikelihood=-3.6716177499083407\t1.0\n",
      "  7:  ... loglikelihood=-3.3871687442184633\t1.0\n",
      "  8:  ... loglikelihood=-3.1500333330426806\t1.0\n",
      "  9:  ... loglikelihood=-2.948441846322817\t1.0\n",
      " 10:  ... loglikelihood=-2.7742780498825166\t1.0\n",
      " 11:  ... loglikelihood=-2.6217774354777066\t1.0\n",
      " 12:  ... loglikelihood=-2.486736217459275\t1.0\n",
      " 13:  ... loglikelihood=-2.3660158578319015\t1.0\n",
      " 14:  ... loglikelihood=-2.2572235522844273\t1.0\n",
      " 15:  ... loglikelihood=-2.1585005894092504\t1.0\n",
      " 16:  ... loglikelihood=-2.068378707719667\t1.0\n",
      " 17:  ... loglikelihood=-1.985680431946904\t1.0\n",
      " 18:  ... loglikelihood=-1.9094485299385593\t1.0\n",
      " 19:  ... loglikelihood=-1.838895167756793\t1.0\n",
      " 20:  ... loglikelihood=-1.7733646507220715\t1.0\n",
      " 21:  ... loglikelihood=-1.7123057023089139\t1.0\n",
      " 22:  ... loglikelihood=-1.655250548562612\t1.0\n",
      " 23:  ... loglikelihood=-1.6017989315031747\t1.0\n",
      " 24:  ... loglikelihood=-1.5516057420135616\t1.0\n",
      " 25:  ... loglikelihood=-1.5043713448620364\t1.0\n",
      " 26:  ... loglikelihood=-1.4598339301359313\t1.0\n",
      " 27:  ... loglikelihood=-1.4177634070960166\t1.0\n",
      " 28:  ... loglikelihood=-1.3779564844056968\t1.0\n",
      " 29:  ... loglikelihood=-1.3402326719019797\t1.0\n",
      " 30:  ... loglikelihood=-1.3044310048660428\t1.0\n",
      " 31:  ... loglikelihood=-1.270407339728444\t1.0\n",
      " 32:  ... loglikelihood=-1.2380321054928396\t1.0\n",
      " 33:  ... loglikelihood=-1.2071884214605901\t1.0\n",
      " 34:  ... loglikelihood=-1.1777705115842172\t1.0\n",
      " 35:  ... loglikelihood=-1.1496823607326967\t1.0\n",
      " 36:  ... loglikelihood=-1.1228365695721934\t1.0\n",
      " 37:  ... loglikelihood=-1.0971533735562373\t1.0\n",
      " 38:  ... loglikelihood=-1.0725597983362336\t1.0\n",
      " 39:  ... loglikelihood=-1.0489889292276702\t1.0\n",
      " 40:  ... loglikelihood=-1.0263792765547892\t1.0\n",
      " 41:  ... loglikelihood=-1.0046742220114124\t1.0\n",
      " 42:  ... loglikelihood=-0.9838215338163709\t1.0\n",
      " 43:  ... loglikelihood=-0.9637729405585598\t1.0\n",
      " 44:  ... loglikelihood=-0.9444837553328362\t1.0\n",
      " 45:  ... loglikelihood=-0.925912543151189\t1.0\n",
      " 46:  ... loglikelihood=-0.9080208257409711\t1.0\n",
      " 47:  ... loglikelihood=-0.8907728187655186\t1.0\n",
      " 48:  ... loglikelihood=-0.8741351972629315\t1.0\n",
      " 49:  ... loglikelihood=-0.8580768857278238\t1.0\n",
      " 50:  ... loglikelihood=-0.8425688697836601\t1.0\n",
      " 51:  ... loglikelihood=-0.8275840268297128\t1.0\n",
      " 52:  ... loglikelihood=-0.8130969734124884\t1.0\n",
      " 53:  ... loglikelihood=-0.7990839273794419\t1.0\n",
      " 54:  ... loglikelihood=-0.7855225831329994\t1.0\n",
      " 55:  ... loglikelihood=-0.7723919985236345\t1.0\n",
      " 56:  ... loglikelihood=-0.7596724921086491\t1.0\n",
      " 57:  ... loglikelihood=-0.7473455496638342\t1.0\n",
      " 58:  ... loglikelihood=-0.735393738972745\t1.0\n",
      " 59:  ... loglikelihood=-0.7238006320366588\t1.0\n",
      " 60:  ... loglikelihood=-0.7125507339502855\t1.0\n",
      " 61:  ... loglikelihood=-0.7016294177766088\t1.0\n",
      " 62:  ... loglikelihood=-0.6910228648307859\t1.0\n",
      " 63:  ... loglikelihood=-0.6807180098496589\t1.0\n",
      " 64:  ... loglikelihood=-0.6707024905815298\t1.0\n",
      " 65:  ... loglikelihood=-0.6609646013816655\t1.0\n",
      " 66:  ... loglikelihood=-0.6514932504434992\t1.0\n",
      " 67:  ... loglikelihood=-0.6422779203346519\t1.0\n",
      " 68:  ... loglikelihood=-0.6333086315413107\t1.0\n",
      " 69:  ... loglikelihood=-0.624575908754921\t1.0\n",
      " 70:  ... loglikelihood=-0.6160707496620066\t1.0\n",
      " 71:  ... loglikelihood=-0.6077845960217699\t1.0\n",
      " 72:  ... loglikelihood=-0.5997093068372301\t1.0\n",
      " 73:  ... loglikelihood=-0.5918371334444946\t1.0\n",
      " 74:  ... loglikelihood=-0.5841606963614776\t1.0\n",
      " 75:  ... loglikelihood=-0.5766729637523577\t1.0\n",
      " 76:  ... loglikelihood=-0.5693672313774011\t1.0\n",
      " 77:  ... loglikelihood=-0.562237103909765\t1.0\n",
      " 78:  ... loglikelihood=-0.5552764775116275\t1.0\n",
      " 79:  ... loglikelihood=-0.5484795235716122\t1.0\n",
      " 80:  ... loglikelihood=-0.5418406735141543\t1.0\n",
      " 81:  ... loglikelihood=-0.5353546045992754\t1.0\n",
      " 82:  ... loglikelihood=-0.5290162266382529\t1.0\n",
      " 83:  ... loglikelihood=-0.5228206695570472\t1.0\n",
      " 84:  ... loglikelihood=-0.516763271745102\t1.0\n",
      " 85:  ... loglikelihood=-0.5108395691323245\t1.0\n",
      " 86:  ... loglikelihood=-0.505045284941798\t1.0\n",
      " 87:  ... loglikelihood=-0.49937632007003485\t1.0\n",
      " 88:  ... loglikelihood=-0.4938287440504932\t1.0\n",
      " 89:  ... loglikelihood=-0.48839878655961677\t1.0\n",
      " 90:  ... loglikelihood=-0.48308282942786\t1.0\n",
      " 91:  ... loglikelihood=-0.477877399121137\t1.0\n",
      " 92:  ... loglikelihood=-0.4727791596607778\t1.0\n",
      " 93:  ... loglikelihood=-0.46778490595254396\t1.0\n",
      " 94:  ... loglikelihood=-0.4628915574974931\t1.0\n",
      " 95:  ... loglikelihood=-0.4580961524595205\t1.0\n",
      " 96:  ... loglikelihood=-0.4533958420662958\t1.0\n",
      " 97:  ... loglikelihood=-0.44878788532201885\t1.0\n",
      " 98:  ... loglikelihood=-0.44426964401200864\t1.0\n",
      " 99:  ... loglikelihood=-0.43983857798058357\t1.0\n",
      "100:  ... loglikelihood=-0.4354922406650239\t1.0\n"
     ]
    }
   ],
   "source": [
    "try (InputStream dataInputStream = new FileInputStream(\"../data/en-frograt.train\")) {\n",
    "    // Create input stream for training data\n",
    "    ObjectStream<String> objectStream = new PlainTextByLineStream(dataInputStream, StandardCharsets.UTF_8);\n",
    "    ObjectStream<DocumentSample> documentSampleStream = new DocumentSampleStream(objectStream);\n",
    "    // train the model\n",
    "    DoccatModel documentCategorizationModel = DocumentCategorizerME.train(\"en\", documentSampleStream);\n",
    "    OutputStream modelOutputStream = new BufferedOutputStream(new FileOutputStream(new File(\"../models/en-frograt.bin\")));\n",
    "    // Serialize the model\n",
    "    OutputStream modelBufferedOutputStream = new BufferedOutputStream(modelOutputStream);\n",
    "    documentCategorizationModel.serialize(modelBufferedOutputStream);\n",
    "    \n",
    "} catch (FileNotFoundException e) {\n",
    "    // Handle exceptions\n",
    "    System.out.println(\"Can't find files!\");\n",
    "} catch (IOException e) {\n",
    "    // Handle exceptions\n",
    "    System.out.println(\"Something off here!\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------\n",
      "Category : Probability\n",
      "---------------------------------\n",
      "frog : 0.5797537229352753\n",
      "rat : 0.42024627706472467\n",
      "---------------------------------\n",
      "\n",
      "frog : is the predicted category for the given sentence.\n"
     ]
    }
   ],
   "source": [
    "// Testing the model\n",
    "\n",
    "try (InputStream modelInputStream = new FileInputStream(\"../models/en-frograt.bin\")) {\n",
    "    // Create input stream for training data\n",
    "    DoccatModel model = new DoccatModel(modelInputStream);\n",
    "    DocumentCategorizerME myCategorizer = new DocumentCategorizerME(model);\n",
    "    String[] docWords = \"This amphibious animal makes a ribbidty sound. It also lives in both water and land. It's cold blooded one.\".replaceAll(\"[^A-Za-z]\", \" \").split(\" \");\n",
    "    double[] aProbs = myCategorizer.categorize(docWords);\n",
    "    String predictedCategory = myCategorizer.getBestCategory(aProbs);\n",
    "    \n",
    "    System.out.println(\"\\n---------------------------------\\nCategory : Probability\\n---------------------------------\");\n",
    "    for(int i=0; i<myCategorizer.getNumberOfCategories(); i++){\n",
    "        System.out.println(myCategorizer.getCategory(i) + \" : \"+ aProbs[i]);\n",
    "    }\n",
    "    System.out.println(\"---------------------------------\");\n",
    "\n",
    "    System.out.println(\"\\n\"+ predictedCategory +\" : is the predicted category for the given sentence.\");\n",
    "\n",
    "} catch (FileNotFoundException e) {\n",
    "    // Handle exceptions\n",
    "    System.out.println(\"Can't find files!\");\n",
    "} catch (IOException e) {\n",
    "    // Handle exceptions\n",
    "    System.out.println(\"Something off here!\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For sample: 0\n",
      "Sentence  : This amphibious animal makes a ribbidty sound. It also lives in both water and land. It's cold blooded one.\n",
      "Prob ratio: frog[0.5798]  rat[0.4202]\n",
      "Predicted : frog\n",
      "Accuracy  : 1.0\n",
      "\n",
      "For sample: 1\n",
      "Sentence  : The fur of the rodent is very smooth and white. It nurses its pups for 21 days until it continues to live.\n",
      "Prob ratio: frog[0.1693]  rat[0.8307]\n",
      "Predicted : rat\n",
      "Accuracy  : 1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "//\n",
    "try (InputStream modelInputStream = new FileInputStream(\"../models/en-frograt.bin\")){\n",
    "    DoccatModel model = new DoccatModel(modelInputStream);\n",
    "    DocumentCategorizerME myCategorizer = new DocumentCategorizerME(model);\n",
    "    \n",
    "    DocumentCategorizerEvaluator evaluator = new DocumentCategorizerEvaluator(myCategorizer);\n",
    "    \n",
    "    String category[] = {\"frog\",\"rat\"};\n",
    "    String content[] = {\"This amphibious animal makes a ribbidty sound. It also lives in both water and land. It's cold blooded one.\",\n",
    "                        \"The fur of the rodent is very smooth and white. It nurses its pups for 21 days until it continues to live.\"};\n",
    "    for (int i=0; i<category.length; i++) {\n",
    "        double[] probability = myCategorizer.categorize(content[i]);\n",
    "        DocumentSample sample = new DocumentSample(category[i], content[i]);\n",
    "        evaluator.evaluteSample(sample);\n",
    "        double result = evaluator.getAccuracy();\n",
    "        System.out.println(\"For sample: \" + i );\n",
    "        System.out.println(\"Sentence  : \" + content[i]);\n",
    "        System.out.println(\"Prob ratio: \" + myCategorizer.getAllResults(probability));\n",
    "        System.out.println(\"Predicted : \" + category[i]);    \n",
    "        System.out.println(\"Accuracy  : \" + result + \"\\n\");\n",
    "    }\n",
    "    \n",
    "} catch (FileNotFoundException e) {\n",
    "    System.out.println(\"Can't find model files\");\n",
    "} catch (IOException e) {\n",
    "    System.out.println(\"Something off here!\");\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using stanford API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: problems summary ::\n",
      ":::: ERRORS\n",
      "\tunknown resolver null\n",
      "\n",
      "\tunknown resolver null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%loadFromPOM\n",
    "<!-- https://mvnrepository.com/artifact/edu.stanford.nlp/stanford-corenlp -->\n",
    "<dependency>\n",
    "    <groupId>edu.stanford.nlp</groupId>\n",
    "    <artifactId>stanford-corenlp</artifactId>\n",
    "    <version>3.9.2</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.BufferedReader;\n",
    "import java.io.BufferedWriter;\n",
    "import java.io.File;\n",
    "import java.io.FileNotFoundException;\n",
    "import java.io.FileReader;\n",
    "import java.io.FileWriter;\n",
    "import java.io.IOException;\n",
    "import edu.stanford.nlp.classify.Classifier;\n",
    "import edu.stanford.nlp.classify.ColumnDataClassifier;\n",
    "import edu.stanford.nlp.ling.Datum;\n",
    "import edu.stanford.nlp.objectbank.ObjectBank;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ColumnDataClassifier columnDataClassifier = new ColumnDataClassifier(\"../data/FlowersAndSpices.prop\");\n",
    "Classifier<String, String> classifier = columnDataClassifier.makeClassifier(\n",
    "                                            columnDataClassifier.readTrainingExamples(\n",
    "                                                \"../data/FlowersAndSpices.train\"));\n",
    "\n",
    "// Test the model\n",
    "ObjectBaString> objectBank = ObjectBank.getLineIterator(\"../data/FlowersAndSpices.test\", \"utf-8\");\n",
    "for (String line : objectBank) {\n",
    "    Datum<String, String> datum = columnDataClassifier.makeDatumFromLine(line);\n",
    "    System.out.println(\"Datum: [\" + line + \"]\\tPredicted Category: \" +  \n",
    "        classifier.classOf(datum));\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// To test single text sample\n",
    "String testItem[] = {\"2\",\"Dill Pollen\"};\n",
    "Datum<String, String> datum = columnDataClassifier.makeDatumFromStrings(testItem);\n",
    "System.out.println(\"[\" + testItem[0] + \"\\t\" + testItem[1] + \n",
    "    \"] Predicted Category: \" + classifier.classOf(datum));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using LINGPipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%loadFromPOM\n",
    "<!-- https://mvnrepository.com/artifact/de.julielab/aliasi-lingpipe -->\n",
    "<dependency>\n",
    "    <groupId>de.julielab</groupId>\n",
    "    <artifactId>aliasi-lingpipe</artifactId>\n",
    "    <version>4.1.0</version>\n",
    "</dependency>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import com.aliasi.classify.Classification;\n",
    "import com.aliasi.classify.Classified;\n",
    "import com.aliasi.classify.DynamicLMClassifier;\n",
    "import com.aliasi.lm.NGramProcessLM;\n",
    "import com.aliasi.util.AbstractExternalizable;\n",
    "import com.aliasi.util.Compilable;\n",
    "import com.aliasi.util.Files;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Categories in training file\n",
    "String[] categories = { \"soc.religion.christian\", \n",
    "    \"talk.religion.misc\", \"alt.atheism\", \"misc.forsale\" };"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Setup training directories\n",
    "int nGramSize = 6;\n",
    "DynamicLMClassifier<NGramProcessLM> dynamicLMClassifier = \n",
    "    DynamicLMClassifier.createNGramProcess(categories, nGramSize);\n",
    "final String rootDirectory = \"../data\";\n",
    "final File trainingDirectory = new File(rootDirectory +  \n",
    "   \"/fourNewsGroups/4news-train\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Access training file\n",
    "\n",
    "for (int i = 0; i < categories.length; ++i) {\n",
    "    final File trainingFilesDirectory = new File(trainingDirectory, categories[i]);\n",
    "    String[] trainingFiles = trainingFilesDirectory.list();\n",
    "    for (int j = 0; j < trainingFiles.length; ++j) {\n",
    "\n",
    "        try {\n",
    "            File trainingFile = new File(trainingFilesDirectory, trainingFiles[j]);\n",
    "            String trainingText = Files.readFromFile(trainingFile, \"ISO-8859-1\");\n",
    "\n",
    "            // Train the model\n",
    "            Classification classification = new Classification(categories[i]);\n",
    "            Classified<CharSequence> classified = new Classified<>((CharSequence) trainingText, classification);\n",
    "            // the actual training \n",
    "            dynamicLMClassifier.handle(classified);\n",
    "\n",
    "        } catch (IOException ex) {\n",
    "            // Handle exceptions\n",
    "            System.out.println(\"Can't find files or folders\");\n",
    "        }\n",
    "    }\n",
    "    // Serialize the model\n",
    "    try {\n",
    "        AbstractExternalizable.compileTo((Compilable) dynamicLMClassifier, new File(\"../models/classificationModel.model\"));\n",
    "    } catch (IOException ex) {\n",
    "        // Handle exceptions\n",
    "        System.out.println(\"Can't find model to serialize\");\n",
    "    }\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import java.io.File;\n",
    "import java.io.IOException;\n",
    "import com.aliasi.classify.JointClassification;\n",
    "import com.aliasi.classify.LMClassifier;\n",
    "import com.aliasi.util.AbstractExternalizable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "String[] categories = { \"soc.religion.christian\", \"talk.religion.misc\", \n",
    "    \"alt.atheism\", \"misc.forsale\" };\n",
    "\n",
    "String sampleText = \"An ancient tradition of philosophy and \" +\n",
    "    \"belief rooted in Chinese worldview\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For this text: An ancient tradition of philosophy and belief rooted in Chinese worldview\n",
      "Best Category: talk.religion.misc\n"
     ]
    }
   ],
   "source": [
    "try {\n",
    "    LMClassifier lmClassifier = (LMClassifier)  AbstractExternalizable.readObject(\n",
    "                                        new File(\"../models/classificationModel.model\"));\n",
    "    JointClassification jointClassification = lmClassifier.classify(sampleText);\n",
    "    \n",
    "    String bestCategory = jointClassification.bestCategory();\n",
    "    System.out.println(\"For this text: \" + sampleText);\n",
    "    System.out.println(\"Best Category: \" + bestCategory);\n",
    "} catch (IOException | ClassNotFoundException ex) {\n",
    "    System.out.println(\"Can't find model file\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: talk.religion.misc     Score: -2.49 jointLog2Probability: -186.64\n",
      "Category: alt.atheism            Score: -2.51 jointLog2Probability: -188.06\n",
      "Category: soc.religion.christian Score: -2.85 jointLog2Probability: -213.85\n",
      "Category: misc.forsale           Score: -3.06 jointLog2Probability: -229.35\n"
     ]
    }
   ],
   "source": [
    "// Showing more details to prediction\n",
    "try {\n",
    "    LMClassifier lmClassifier = (LMClassifier)  AbstractExternalizable.readObject(\n",
    "                                        new File(\"../models/classificationModel.model\"));\n",
    "    JointClassification jointClassification = lmClassifier.classify(sampleText);\n",
    "    \n",
    "    for (int i = 0; i < categories.length; i++) {\n",
    "        double score = jointClassification.score(i);\n",
    "        double probability = jointClassification.jointLog2Probability(i);\n",
    "        String category = jointClassification.category(i);\n",
    "        System.out.printf(\"Category: %-22s Score: %4.2f jointLog2Probability: %4.2f%n\", \n",
    "            category, score, probability);\n",
    "    }\n",
    "} catch (IOException | ClassNotFoundException ex) {\n",
    "    System.out.println(\"Can't find model file\");\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Java",
   "language": "java",
   "name": "java"
  },
  "language_info": {
   "codemirror_mode": "java",
   "file_extension": ".jshell",
   "mimetype": "text/x-java-source",
   "name": "Java",
   "pygments_lexer": "java",
   "version": "14.0.1+7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
